# Runbook: Executing Backtests

This runbook provides instructions for executing backtests using the project's CLI.

## 1. Basic Backtest

To run a simple backtest for a configured pair, use the `run` command:

```bash
python src/run_backtest.py run --pair usdcad_wti --start 2018-01-01 --end 2023-12-31
```

This will use the default parameters defined in `configs/pairs.yaml` and `configs/risk.yaml`.

## 2. Backtest with ML Filter

To run a backtest with the ML trade filter enabled, you can either enable it in `configs/pairs.yaml` or use the command-line overrides.

### Using CLI Overrides

This is useful for quick experiments without modifying config files.

```bash
python src/run_backtest.py run \
    --pair usdcad_wti \
    --start 2018-01-01 \
    --end 2023-12-31 \
    --use-ml-filter \
    --ml-model-path "artifacts/models/usdcad_wti/20250918_120000/model.joblib" \
    --ml-threshold 0.70
```

## 3. Building an ML Dataset

Before you can use the ML filter, you need to build a dataset and train a model.

**Step 1: Build the dataset**

```bash
python -m src.ml.dataset \
    --pair usdcad_wti \
    --start 2015-01-01 \
    --end 2024-12-31 \
    --h 20 \
    --rr 1.5
```

This will create `train.parquet` and `val.parquet` files in `data/ml/usdcad_wti/`.

**Step 2: Train the model**

```bash
python src/ml/train.py \
    --pair usdcad_wti \
    --cv 5 \
    --seed 42 \
    --out artifacts/models/usdcad_wti
```

This will train a model and save the artifacts (including the `.joblib` file) to a new timestamped directory inside `artifacts/models/usdcad_wti/`.

## 4. Stress Tests

To run a series of stress tests (e.g., with higher costs or slippage), use the `run_stress_pack.py` script.

```bash
python scripts/reports/run_stress_pack.py \
    --pair usdcad_wti \
    --start 2018-01-01 \
    --end 2023-12-31 \
    --out Docs/reports/stress_test_results.md
```
*(Note: The stress test script is currently a placeholder).*

## 5. Comparing Runs

To compare the results of multiple backtests, use the `compare_runs.py` script. You need to provide the paths to the `summary.json` files generated by each run.

```bash
python scripts/reports/compare_runs.py \
    --runs "Baseline:reports/usdcad_wti/run_id_1/summary.json" \
    --runs "ML_Filtered:reports/usdcad_wti/run_id_2/summary.json" \
    --out Docs/reports/comparison.md
```
